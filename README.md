# ğŸ› ï¸ Amazon Product Data ETL Pipeline using AWS Glue & Redshift

An efficient **ETL (Extract, Transform, Load)** pipeline built on **AWS** to clean and process raw e-commerce product data from Amazon for advanced analytics and business intelligence.

---

## ğŸ“Œ Project Overview

In the era of **data-driven decision-making**, raw e-commerce datasets often contain inconsistencies, null values, and non-standard formats that hinder meaningful insights.

This project leverages AWS services such as:
- **AWS Glue**
- **AWS Glue Crawlers**
- **Amazon S3**
- **Amazon Redshift**

to build an end-to-end **ETL pipeline** that extracts Amazon product data, applies data cleaning and transformations, and loads the high-quality structured data into Redshift for fast querying and reporting.

---

## ğŸ“Š Dataset

**Dataset Name:** Amazon Products Dataset  
**Size:** 1000 records Ã— 24 attributes  
**Purpose:** Structured Amazon product listings for sales and performance analysis  

### ğŸ”‘ Key Attributes
- **Product Metadata:** title, brand, description  
- **Pricing:** final_price, currency  
- **Performance Metrics:** reviews_count, rating, answered_questions  
- **Media Info:** images_count, video_count, image_url  
- **Additional:** asin, seller_id, manufacturer, availability  

---

## âš™ï¸ ETL Pipeline

### ğŸ“¥ Extraction

- Raw Amazon product data (CSV) is uploaded to an **Amazon S3 bucket** (acts as a Data Lake).
- An **AWS Glue Crawler** scans the S3 path, infers the schema, and catalogs metadata in the **AWS Glue Data Catalog**.

### ğŸ§¹ Transformation

A custom **AWS Glue Job** (PySpark) applies transformation logic:

| Attribute          | Transformation Logic |
|-------------------|----------------------|
| **Availability**  | - "Only X left" â†’ "Limited"  <br> - Variants of "in stock" â†’ "In Stock" |
| **Number of Sellers** | Missing â†’ `1` |
| **Domain**         | Short names â†’ full URLs <br> Missing â†’ `https://amazon.com/` |
| **Manufacturer**   | Missing â†’ `unknown` |
| **Plus Content**   | Null â†’ `False` |
| **Top Review**     | Missing â†’ `"no review"` |
| **Null Handling**  | Drop rows/columns with all nulls |
| **Type Conversion**| Convert string/scientific notations â†’ `double` format |

- **Output Format:** Parquet  
- **Transformation Destination:** Amazon S3

### ğŸšš Load

- Transformed Parquet files are loaded into **Amazon Redshift**.
- A **custom schema** ensures optimal column mapping and query performance.
- Data is loaded via the Redshift `COPY` command from the S3 source.

---
## ğŸ” ETL Pipeline Flow

1. **Data Upload (Extract)**
   - Raw Amazon product data (CSV format) is uploaded to an **Amazon S3 bucket**.

2. **Schema Inference**
   - An **AWS Glue Crawler** scans the S3 data and infers the schema.
   - Metadata is stored in the **AWS Glue Data Catalog**.

3. **Data Transformation (Transform)**
   - An **AWS Glue Job** is configured to:
     - Clean and standardize fields (e.g., availability, manufacturer, etc.)
     - Handle null/missing values
     - Convert domains and data types
   - Transformed data is written back to **Amazon S3** in **Parquet format**.

4. **Data Loading (Load)**
   - **Amazon Redshift** is used as the final data warehouse.
   - Data is loaded into Redshift tables via the **COPY command** from the S3 Parquet files.

## ğŸ’» Technologies Used

- **Amazon S3** â€“ Data lake for storing raw and transformed data
- **AWS Glue Crawler** â€“ Automated schema inference and metadata generation
- **AWS Glue** â€“ ETL jobs for data cleaning and transformation
- **AWS Glue Data Catalog** â€“ Centralized metadata repository
- **Amazon Redshift** â€“ Data warehouse for analytics and querying
- **Parquet Format** â€“ Efficient columnar storage format for transformed data
- **Python (PySpark in Glue)** â€“ Custom transformation logic in ETL jobs

---

## ğŸ¯ Use Case

In the era of data-driven business intelligence, raw e-commerce product data is often noisy, incomplete, or inconsistently formatted. This project demonstrates a scalable and automated ETL pipeline using AWS services to process Amazon product listings.

The system:
- Extracts raw CSV data from Amazon S3
- Uses AWS Glue Crawler to infer schema and catalog metadata
- Applies cleaning rules (e.g., fixing availability, filling missing values, type conversions) via AWS Glue Job
- Loads the cleaned data in Parquet format to S3 and finally into Amazon Redshift
- Enables high-performance analytics and dashboarding with clean, structured product data

---

## ğŸ“½ï¸ Demo Video

â–¶ï¸ [Click here to watch the project demo](https://drive.google.com/file/d/1oK8l6ipLLjtEj4OtIGCxEkz0WyWjhyLf/view)

